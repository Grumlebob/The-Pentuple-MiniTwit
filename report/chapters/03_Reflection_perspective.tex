\section{Reflection Perspective}

\subsection{Biggest issues}

These are some of the problems we spent the most time solving.

\subsubsection{Migrations and \texttt{Docker Compose}}

In the beginning of the project after we had refactored \texttt{MiniTwit} to \texttt{.NET},
we ran the program with \texttt{dotnet run}. 
When the \texttt{API} started up it would also perform a database migration,
in case the model had changed. This is a part of \texttt{EF Core}
that makes it easy to evolve the database schema.
However, when we started using \texttt{Docker Compose}, it would not run the migration.
We spent a lot of time trying to make it work.
Additionally, we did not have much experience with \texttt{Docker},
resulting in us spending over 30 hours total on the ticket
``Dockerize application''.

As a solution we ended up creating a separate \texttt{Dockerfile} for the migration.
The sole purpose of this file was to use the \texttt{API} code project to run the migration.
Later on as we implemented the \texttt{CI/CD} pipeline, 
the migration was made part of the deployment process.

\subsubsection{\texttt{Docker Swarm}}

Before we implemented \texttt{Docker Swarm}, we had a single droplet 
and a single \texttt{Docker Compose} file that defined all the MiniTwit modules.
This made it quite the challenge to change to several droplets with 
seperate \texttt{Docker Compose} files for each module.
With a single \texttt{compose} file it was easy to have health checks and
dependencies between the modules. 
Fortunately, the only crucial dependency was the database migration.
The migration cannot happen before the database is up and running.
However, with the database continuously running on its own droplet,
it is no longer an issue.

Another related issue was where to run the migration.
Since the \texttt{API} module is the one communicating with the database,
we initially included it in the \texttt{API Docker Compose} file.
However, this was an issue now that the API was running on several 
worker droplets as part of a \texttt{Docker Swarm}. Workers are designed
to be able to crash and start up again, 
meaning it is unpredictable when they run their given services.
We want to only run the migration once, when we deploy.
Otherwise, multiple concurrent migrations could cause 
race conditions or similar issues on the database.
This was solved by separating the migration into its own docker compose file.
This is run once on the leader node, when we deploy.
We ended up spending over 14 hours on the ticket ``Set up \texttt{Docker Swarm}''.

\subsection{Lesssons learned}

\subsubsection{``It works on my computer''}

We learned the importance of having an environment that is 
self-contained, so it can run across platforms.
The computers used in the group were a mix of \texttt{Mac} and \texttt{Windows},
and the \texttt{droplets} were running \texttt{Ubuntu}. 
Using \texttt{Docker} environments made it easy to run the same program
on different machines without too much trouble.

\subsubsection{DevOps vs ClickOps}

\texttt{Automation} was a big focus in this project.
There were numerous things in this project that 
would have been bottlenecks if we had to do them manually every time.
Especially the \texttt{deployment process} required a lot of steps
of copying files and \texttt{SSH}'ing into \texttt{droplets} to run commands.
Not only is this time consuming, it is also prone to errors.
Forgetting a single step would cause most of the program to not work.
An example of this was when we manually created a \texttt{droplet} and forgot to add the 
group \texttt{SSH keys}. We were then unable to access the \texttt{droplet} and had to 
tear it down and create a new one.

\subsection{Unresolved issues}

There are some issues that we have still not managed to solve.

\subsubsection{Domain \texttt{URL} has long load time}

For some unknown reason, when going to the \texttt{Client} with the \texttt{HTTPS} domain name, 
it loads for several seconds before the page is shown.
We did spend some time trying to find the cause,
but we could not find anything.

\subsubsection{A hidden dependency}

The \texttt{API} must allow the \texttt{Client} to access it by 
setting the \texttt{CORS} (Cross-Origin Resource Sharing) policy.
We do this in the \texttt{appsettings}. The problem is that it is done manually.
When the \texttt{Client} changes \texttt{IP-address}, we must include 
that \texttt{IP} in the \texttt{CORS} policy of the \texttt{API}.
We could allow all origins to avoid this issue, 
but optimally we would have liked to dynamically 
inject it somewhere in the workflow.

\subsubsection{Seq limitations}

Adding monitoring with Seq was easy, since it is made specifically for .NET.
However, it has some limitations, regarding graphs.
It has a specific format for the SQL queries you can make graphs from.
Specifically, it can only contain the following operations.
\texttt{select, where, group by, having, order by, limit}.
We wanted to make a business relevant graph testing the 1\% rule\cite{1_perc_rule}.
In short, group users based on how many posts they have made.
In order to combine this data, we only found ways that involve 
\texttt{join} operations, which Seq does not support.

Furthormore, we have not found a way to save the graphs we make.
This means, if we tear down the droplet running Seq,
we lose all the custom graphs we have made.
To avoid starting completely over, we have saved the queries,
so they can be manually pasted back in, when we run Seq again.

\subsubsection{File structure}

Terraform was one of the last things we added to the project.
Currently, the terraform files are placed in the ``MiniTwitSoultion'' 
folder next to ``remote\_files'' which are the files copied to the droplets.
If the project were to continue, we would move these things to a deployment folder.

\subsection{The DevOps difference}

These are the things that made this project different and more devopsy
than other projects we have worked on.

\subsubsection{Continuous integration/deployment}

Deployment was a big focus in this project, 
which is something we are not very used to.
The issues we spent the most time on were often related to deployment.
However, once the workflow was in place, it was
an awesome advantage to be able to push some code,
whereafter it was automatically available for everyone on the url, 
meaning there was no need to run the program locally to see the results.

\subsubsection{Monitoring}

Monitoring is also something we haven't used much in previous projects.
But given how easy it was to add, and the advantages it gave,
we will definitely include it in future projects.
It was super useful for detecting errors, unexpected behavior,
and slow response times, as well as general traffic to see 
what kind of requests the program receives.
This helps us make systematic optimizations. For instance,
we considered optmizing our caching to speed up the API response times.
However, the monitor revealed that the number of writes were
greater than expected compared to the number of reads,
meaning there would be too much cache invalidation for it to be worth it.
At one point we also accidentally created a droplet on with an American server.
We could immediately tell from the monitor that the response 
times were much higher because of this.
We quickly changed it back to the European server 
that the other droplets were running with.

\subsection{Reasons for tool choices}

.NET, minimal api, Blazor,

Docker

Digital ocean.

Seq, serilog

Terraform

Csharpier

SonarQube

Postgres

dependabot

coveralls

testcontainers

github actions